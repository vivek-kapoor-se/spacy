# -*- coding: utf-8 -*-
"""IndiaAI_Spacy.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ykSZzTlWpcHc0LTHavSEbTbASqVtlzzw
"""
# https://indiaai.s3.ap-south-1.amazonaws.com/docs/test.csv
# https://indiaai.s3.ap-south-1.amazonaws.com/docs/train.csv

import spacy
from spacy.training.example import Example
import pandas as pd

# Load train and test data
train_data = pd.read_csv("train.csv")
test_data = pd.read_csv("test.csv")

def prepare_data(df):
    spacy_data = []
    for _, row in df.iterrows():
        text = row["crimeaditionalinfo"]
        # Dictionary of categories as required by SpaCy for multi-label classification
        cats = {
            "CATEGORY_" + str(row["category"]): True,  # Prefix to avoid label conflicts
            "SUB_CATEGORY_" + str(row["sub_category"]): True
        }
        spacy_data.append((text, {"cats": cats}))
    return spacy_data

# Prepare training and test data
train_spacy_data = prepare_data(train_data)
test_spacy_data = prepare_data(test_data)

# Initialize a blank SpaCy model and add TextCategorizer with multilabel support
nlp = spacy.blank("en")
textcat = nlp.add_pipe("textcat_multilabel")

# Add labels for all categories and sub-categories
for _, annotations in train_spacy_data:
    for label in annotations["cats"]:
        textcat.add_label(label)

# # Initialize optimizer
# optimizer = nlp.initialize()

# # Training loop
# n_epochs = 10
# for epoch in range(n_epochs):
#     losses = {}
#     for text, annotations in train_spacy_data:
#         if not isinstance(text, str):
#             continue
#         doc = nlp.make_doc(text)
#         example = Example.from_dict(doc, annotations)
#         # Update the model
#         nlp.update([example], sgd=optimizer, losses=losses)
#     print(f"Loss at epoch {epoch}: {losses}")

# Initialize optimizer
optimizer = nlp.initialize()

# Ensure SpaCy is set to use the GPU
# spacy.require_gpu()

# Training loop
n_epochs = 10

for epoch in range(n_epochs):
    print(f"Starting epoch {epoch + 1}")
    losses = {}

    for i, (text, annotations) in enumerate(train_spacy_data):
        if not isinstance(text, str):
            continue
        # Prepare the example for training
        doc = nlp.make_doc(text)
        example = Example.from_dict(doc, annotations)

        # Update the model
        nlp.update([example], sgd=optimizer, losses=losses)

        # Display the current loss every 100 examples
        if (i + 1) % 100 == 0:
            print(f"Processed {i + 1} examples, Current loss: {losses['textcat_multilabel']:.4f}")

    # Print loss at the end of each epoch
    print(f"Loss at the end of epoch {epoch + 1}: {losses}")

# Evaluation
correct = 0
total = len(test_spacy_data)

for text, annotations in test_spacy_data:
    doc = nlp(text)
    cats_pred = doc.cats
    cats_true = annotations["cats"]

    # Check if predicted categories match the actual categories
    correct += all((cats_pred[label] >= 0.5) == cats_true[label] for label in cats_true)

accuracy = correct / total
print(f"Accuracy: {accuracy * 100:.2f}%")